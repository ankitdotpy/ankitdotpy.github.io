---
layout: default
---

## About Ankit
I am currently working as Machine Learning Intern at [Nurix.ai](https://nurix.ai), where I am working on some intersting stuff in voice ai. Prior to this, I was a Deep Learning Engineer at [TensorGo Software](https://www.tensorgo.com/), working majorly on machine learning, llms, and data analysis. I graduated from Indian Institute of Information Technology, Gwalior in 2024 with a Bachelor's degree in Information Technology.

My areas of interest are building efficient machine learning systems such as distributed training and inference, low resource NLP, and machine learning in general. I also want to get into amateur robotics, once I have other things sorted out. Currently trying my hands at blogging to share my long form thoughts on Programming and AI.

Always on lookout to work on intersting project..

## My May 2025 Reading List

- [X] [Attention Is All You Need](https://lnkd.in/gF2X6HbE)
- [ ] [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://lnkd.in/gRDQpkzw)
- [ ] [Proximal Policy Optimization Algorithms](https://lnkd.in/g7NX8mPr)
- [ ] [Deep Reinforcement Learning from Human Preferences](https://lnkd.in/gDrDVMHG)
- [ ] [FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](https://arxiv.org/abs/2407.08608)
- [ ] [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)

<!-- ### 2018
- [ ] [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://lnkd.in/gfVWKUjJ)
- [ ] [Deep contextualized word representations (ELMo)](https://lnkd.in/gK3szKDM)
- [ ] [Improving Language Understanding by Generative Pre-Training (GPT)](https://lnkd.in/gbcJETKy)
- [ ] [SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing](https://lnkd.in/g7B_xu9g)
- [ ] [Universal Language Model Fine-tuning for Text Classification (ULMFiT)](https://lnkd.in/guiMi8DE)

### 2019
- [ ] [Language Models are Unsupervised Multitask Learners (GPT-2)](https://lnkd.in/gYq2tVvr)
- [ ] [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://lnkd.in/gsvSJmh7)
- [ ] [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://lnkd.in/gF5Z4A2y)
- [ ] [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://lnkd.in/ggqZdfmZ)
- [ ] [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://lnkd.in/gK4ENGFU)
- [ ] [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://lnkd.in/gfEnnf7Y)
- [ ] [Generating Long Sequences with Sparse Transformers](https://lnkd.in/gjbsWemR)

### 2020
- [ ] [Reformer: The Efficient Transformer](https://lnkd.in/gz8_Ftkq)
- [ ] [Longformer: The Long-Document Transformer](https://lnkd.in/gNBJ9Qtc)
- [ ] [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://lnkd.in/gJTFfgcm)
- [ ] [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://lnkd.in/gyS3BX7q)
- [ ] [Big Bird: Transformers for Longer Sequences](https://lnkd.in/gYZuKxKk)
- [ ] [GPT-3](https://lnkd.in/gFVBFQap)
- [ ] [Rethinking Attention with Performers](https://lnkd.in/gTtdQw7w)
- [ ] [T5](https://lnkd.in/gSNZnntD)
- [ ] [Measuring Massive Multitask Language Understanding](https://lnkd.in/geGYGgRx)
- [ ] [ZeRO (Zero Redundancy Optimizer)](https://lnkd.in/gp-bTGmM)
- [ ] [ELECTRA](https://lnkd.in/gC9QNxvA)
- [ ] [Scaling Laws](https://lnkd.in/gHvNumQd)

### 2021
- [ ] [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
- [ ] [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473)
- [ ] [Transcending Scaling Laws with 0.1% Extra Compute](https://arxiv.org/abs/2210.11399)
- [ ] [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426)
- [ ] [LoRA](https://arxiv.org/abs/2106.09685)
- [ ] [Gopher](https://arxiv.org/abs/2112.11446)
- [ ] [Megatron-Turing NLG](https://arxiv.org/abs/2201.11990)

### 2022
- [ ] [EFFICIENTLY SCALING TRANSFORMER INFERENCE](https://arxiv.org/pdf/2211.05102)
- [ ] [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)
- [ ] [Chinchilla](https://arxiv.org/abs/2203.15556)
- [ ] [Chain-of-thought prompting](https://arxiv.org/abs/2201.11903)
- [ ] [InstructGPT](https://arxiv.org/abs/2203.02155)
- [ ] [BLOOM](https://arxiv.org/abs/2211.05100)
- [ ] [Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)
- [ ] [Flash Attention](https://arxiv.org/abs/2205.14135)
- [ ] [Grouped-query attention](https://arxiv.org/abs/2305.13245)
- [ ] [ALiBi position encoding](https://arxiv.org/abs/2108.12409)
- [ ] [DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale](https://arxiv.org/abs/2207.00032)
- [ ] [Red Teaming Language Models with Language Models](https://arxiv.org/abs/2202.03286)
- [ ] [HELM (Holistic Evaluation of Language Models)](https://arxiv.org/abs/2211.09110)
- [ ] [GPTQ](https://arxiv.org/abs/2210.17323)
- [ ] [Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models](https://arxiv.org/abs/2206.04615)

### 2023
- [ ] [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- [ ] [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
- [ ] [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)
- [ ] [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- [ ] [Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment](https://arxiv.org/abs/2312.12148)
- [ ] [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)
- [ ] [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/abs/2306.00978)
- [ ] [Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442)
- [ ] [Voyager: An Open-Ended Embodied Agent with Large Language Models](https://arxiv.org/abs/2305.16291)
- [ ] [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)
- [ ] [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)
- [ ] [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244)
- [ ] [DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales](https://arxiv.org/abs/2308.01320)
- [ ] [GPT-4](https://arxiv.org/abs/2303.08774)
- [ ] [Mistral 7b](https://arxiv.org/abs/2310.06825)
- [ ] [Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290)
- [ ] [PaLM 2](https://arxiv.org/abs/2305.10403)
- [ ] [LIMA](https://arxiv.org/abs/2305.11206)
- [ ] [Mamba](https://arxiv.org/abs/2312.00752)
- [ ] [LLaVA (Visual Instruction Tuning)](https://arxiv.org/abs/2304.08485)
- [ ] [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463)
- [ ] [Qwen Technical Report](https://arxiv.org/abs/2309.16609)
- [ ] [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://arxiv.org/abs/2308.12966)

### 2024
- [ ] [Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference](https://arxiv.org/abs/2403.04132)
- [ ] [TinyLlama: An Open-Source Small Language Model](https://arxiv.org/abs/2401.02385)
- [ ] [Jamba: A Hybrid Transformer-Mamba Language Model](https://arxiv.org/abs/2403.19887)
- [ ] [Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution](https://arxiv.org/abs/2409.12191)

- [ ] [LLaMA 3](https://arxiv.org/abs/2407.21783) -->

## Publications
Coming Soon...